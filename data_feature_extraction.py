# -*- coding: utf-8 -*-
"""Data Feature Extraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JYnl5oMUwbhD5Wxej-B7qm2eUTJh02L6

1) Preprocess the Audio Files
Normalization: Ensure all audio files are at a consistent volume and sample rate.
Noise Reduction (optional): Apply noise reduction techniques if necessary to clean up the recordings.
Segmentation (if required): If you're working with smaller units than entire files, segment the audio into the desired lengths.

2) Feature Extraction
Feature extraction involves converting audio signals into a form that's easier for machine learning models to process. Common features extracted from audio include:

- Mel-Frequency Cepstral Coefficients (MFCCs): Widely used in speech and emotion recognition, MFCCs provide a representation of the short-term power spectrum of sound.
- Spectral Features: Includes spectral centroid, spectral bandwidth, spectral contrast, and spectral roll-off.
Chroma Features: Relate to the twelve different pitch classes.
- Zero Crossing Rate: The rate at which the signal changes sign.
Energy: The sum of squares of the signal values, normalized by the respective frame length.
- Pitch: The fundamental frequency.
You can use libraries like librosa in Python for extracting these features.

3) Extract Labels
The labels (emotions, intensity, etc.) can be derived from the file names or the accompanying metadata. Parse the file names according to the naming convention detailed in the RAVDESS documentation to extract these labels.

Refer to: https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio/data
"""

import zipfile
import os
import librosa
import pandas as pd
import numpy as np

# Path to the zip file and extraction directory
zip_path = '/content/Archive.zip'
extract_dir = '/content/ravdess_data'

# Extract the audio files from the zip
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

# Function to extract features and return a row
def extract_features(file_path):
    signal, sample_rate = librosa.load(file_path, sr=None)
    mfccs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=13)
    mfccs_processed = np.mean(mfccs.T, axis=0)
    return mfccs_processed

# Function to parse the emotion from the filename
def parse_emotion(filename):
    emotion_code = int(filename.split('-')[2])
    emotions = {
        1: 'neutral',
        2: 'calm',
        3: 'happy',
        4: 'sad',
        5: 'angry',
        6: 'fearful',
        7: 'disgust',
        8: 'surprised'
    }
    return emotions.get(emotion_code, 'unknown')

# Prepare dataset
data = []
for root, dirs, files in os.walk(extract_dir):
    for file in files:
        if file.endswith('.wav') and not file.startswith('.') and '__MACOSX' not in root:
            file_path = os.path.join(root, file)
            try:
                features = extract_features(file_path)
                emotion = parse_emotion(file)
                data.append([file, emotion] + list(features))
            except Exception as e:
                print(f"Error processing {file_path}: {e}")

# Create DataFrame
df = pd.DataFrame(data, columns=['filename', 'label'] + [f'mfcc_{i}' for i in range(13)])

#Checks and removes duplicates
df = df.drop_duplicates(subset='filename', keep='first')

from sklearn.model_selection import train_test_split

# Separate features and label for splitting
X = df.drop(['filename', 'label'], axis=1)
y = df['label']

# Stratified split to ensure equal representation of each label
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)

# Create training and testing sets with filenames and labels
train_df = df.iloc[X_train.index].reset_index(drop=True)
test_df = df.iloc[X_test.index].reset_index(drop=True)

# Save to CSV files
train_df.to_csv('/content/training_set.csv', index=False)
test_df.to_csv('/content/testing_set.csv', index=False)

